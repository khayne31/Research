{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"D:\\Andaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mD:\\Andaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m   \u001b[1;31m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8248739751d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Andaconda\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Andaconda\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Andaconda\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrewriter_config_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Andaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tfe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Andaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 83\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"D:\\Andaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import scipy as sp\n",
    "\n",
    "weights = np.loadtxt(\"weights_tract.txt\") #structural\n",
    "length_matrix = np.loadtxt(\"lengths_tract.txt\")\n",
    "connections = np.loadtxt(\"weights_surf.txt\")\n",
    "averages = np.loadtxt(\"Experiment3/averages.txt\")\n",
    "averages246 = np.loadtxt(\"Experiment3/averages246.txt\")\n",
    "loaded_246 = np.loadtxt(\"trained_weights246.txt\")\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_raw = np.loadtxt(\"depressesddata.txt\", delimiter = \",\")\n",
    "depressed_raw = np.reshape(depressed_raw, (int(np.shape(depressed_raw)[0]/(246)), 246, 149))\n",
    "depressed_data = depressed_raw[0:88,:,:]\n",
    "control_data = depressed_raw[88:,:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "depressed42_connectome = sio.loadmat('CID_42subj_connectomes.mat')['aggregate']\n",
    "depressed42_connectome = np.swapaxes(depressed42_connectome, 2, 0)\n",
    "control_connectome = sio.loadmat('Experiment3/control_connectomes.mat')['aggregate']\n",
    "control_connectome = np.swapaxes(control_connectome, 2, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed42_connectome = np.delete(depressed42_connectome, 18, axis=0)\n",
    "depressed42_connectome = np.delete(depressed42_connectome, 63, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiply length times the weights then add them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = sio.loadmat('depression_connectomes.mat')['aggregate']\n",
    "dd = np.swapaxes(dd, 2, 0)\n",
    "for item in dd:\n",
    "    plt.imshow(item)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "x = sio.loadmat('length-weight.mat')['matrix']\n",
    "x = np.swapaxes(x, 2, 0)\n",
    "x = np.swapaxes(x, 1, 2)\n",
    "\n",
    "plt.imshow(x[100])\n",
    "plt.show() \n",
    "\n",
    "new_lengths = x[0: int(x.shape[0]/2), :, :]\n",
    "new_weights = x[ int(x.shape[0]/2) :, :, :]\n",
    "\n",
    "print(new_weights.shape)\n",
    "\n",
    "avg_new_lengths = np.empty((246,246))\n",
    "avg_new_weights_246 = np.empty((246,246))\n",
    "\n",
    "for i in range(new_lengths.shape[1]):\n",
    "    for j in range(new_lengths.shape[2]):\n",
    "        avg_new_lengths[i][j] = np.average(new_lengths[:, i, j])\n",
    "        \n",
    "        avg_new_weights_246[i][j] = np.average(new_weights[:, i, j])\n",
    "\n",
    "        \n",
    "       \n",
    "        \n",
    "#avg_new_lengths = length_format(avg_new_lengths)\n",
    "plt.imshow(avg_new_weights_246)\n",
    "plt.show()\n",
    "plt.imshow(np.corrcoef(avg_new_lengths))\n",
    "plt.show()\n",
    "plt.imshow(np.swapaxes(new_weights[0], 0 , 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_neural_volume = []\n",
    "for i in range(new_lengths.shape[0]):\n",
    "    total_neural_volume.append(np.linalg.norm(np.multiply(new_lengths[i], new_weights[i])))\n",
    "TNV_minus_median = np.subtract(total_neural_volume, np.median(total_neural_volume))\n",
    "TNV_minus_median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avaliable_matricies = {}\n",
    "#from matricies import *\n",
    "def import_matrix_form_txt(file_name, size, matrix_name):\n",
    "    current_matrix = np.loadtxt(file_name, float, delimiter = \",\")\n",
    "    current_matrix = np.transpose(current_matrix)\n",
    "    current_matrix = np.reshape(current_matrix, size)\n",
    "    current_matrix = np.swapaxes(current_matrix, 0 , 1)\n",
    "    avaliable_matricies[matrix_name] = current_matrix\n",
    "\n",
    "    print(current_matrix.shape)\n",
    "    plt.imshow(np.corrcoef(current_matrix[0,:,:]))\n",
    "    plt.show()\n",
    "\n",
    "#import_matrix_form_txt('midnight.txt', (66, 60, 818), \"midnight\")\n",
    "#avaliable_matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_matrix_depressed = np.loadtxt(\"subj1_lengths246_mean.txt\")\n",
    "depressed_volumetric = np.loadtxt(\"subj1_weights246.txt\")\n",
    "my_weights_loaded = np.loadtxt(\"trained_weights.txt\")\n",
    "my_weights_loaded_236 = np.loadtxt(\"trained_weights246.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "def get_246():\n",
    "    new_data = []\n",
    "    pwd = os.path.join(os.getcwd(), \"new_data\")\n",
    "    for subdir, dirs, files in os.walk(pwd):\n",
    "            for file in files:\n",
    "                if 'cyberduck' not in file:\n",
    "                    print(os.path.join(pwd, file))\n",
    "                    matrix = np.loadtxt(os.path.join(pwd, file))\n",
    "                    matrix = np.reshape(matrix, (int(np.shape(matrix)[0]/246), 246, 1200))\n",
    "                    new_data.append(matrix)                \n",
    "                    sys.stdout.flush()\n",
    "    \n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_246 = get_246()\n",
    "np.shape(large_246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_rest = np.loadtxt(\"all_rest.txt\", float, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_task = np.loadtxt(\"all_task.txt\", float, delimiter = \" \") #has rest and task. USE THIS\n",
    "#all_rest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_rest = np.reshape(all_rest, (66, 447, 2400))\n",
    "# all_rest = np.swapaxes(all_rest, 0 , 1)\n",
    "# #plt.imshow(np.corrcoef(all_rest[0,:,:]))\n",
    "\n",
    "# new_all_rest = all_rest[:, : , :1800]\n",
    "# print(new_all_rest.shape)\n",
    "# all_data = np.concatenate((all_task, new_all_rest), axis = 0)\n",
    "# all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_task = np.reshape(all_task, (66, 447, 1800))\n",
    "# all_task = np.swapaxes(all_task, 0 , 1)\n",
    "# plt.imshow(np.corrcoef(all_task[0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(np.corrcoef(all_task[0,:,:]))\n",
    "# tr = 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_format(lengths):\n",
    "    return np.reshape([x if x != 0 else 250 for x in lengths.flatten() ], np.shape(lengths))\n",
    "\n",
    "length_mod = length_format(length_matrix)\n",
    "depressed_length_mod = length_format(length_matrix_depressed)\n",
    "avg_new_length_mod = length_format(avg_new_lengths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thirty_rest = np.loadtxt(\"30rest.txt\", float, delimiter = \",\")\n",
    "#thirty_rest = np.reshape(thirty_rest, (66,30,1200)) #change for new data\n",
    "#thirty_rest = np.swapaxes(thirty_rest, 0,1)\n",
    "#plt.imshow(np.corrcoef(thirty_rest[0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1200/149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint \n",
    "def sample(all_scans, short = False, desired_length = 1):\n",
    "    if type(all_scans) == list:\n",
    "        number = randint(0,len(all_scans) - 1)\n",
    "        chunk = all_scans[number]\n",
    "        scan_num = randint(0, np.shape(chunk)[0] - 1)\n",
    "        if not short:\n",
    "            return chunk[scan_num], scan_num\n",
    "        else:\n",
    "            shape = np.shape(chunk[scan_num])\n",
    "            print(shape)\n",
    "            return chunk[scan_num][:,::int(np.floor(shape[1]/desired_length))][:,:desired_length], scan_num\n",
    "    else:\n",
    "        number = randint(0, np.shape(all_scans)[0] - 1) # change for new data set\n",
    "        if not short:\n",
    "            return all_scans[number], number\n",
    "        else:\n",
    "            shape = np.shape(all_scans)\n",
    "            return all_scans[number][:,::int(np.floor(shape[1]/desired_length))][:,:desired_length], number\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(depressed_data, short = True, desired_length = 149)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midnight = np.loadtxt(\"midnight.txt\", float, delimiter = \",\")\n",
    "midnight = np.transpose(midnight)\n",
    "midnight = np.reshape(midnight, (66, 60, 818))\n",
    "midnight = np.swapaxes(midnight, 0 , 1)\n",
    "print(midnight.shape)\n",
    "plt.imshow(np.corrcoef(midnight[0,:,:]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "def normalize_matrix(matrix):\n",
    "    w,v = LA.eig(matrix)\n",
    "    return matrix/w[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(SC, data_name, plot = True):\n",
    "    healthy_dist = []\n",
    "    for i in range(np.shape(new_weights)[0]):\n",
    "        corr = np.corrcoef(new_weights[i].flatten(), SC.flatten())[0,1]\n",
    "        healthy_dist.append(corr)\n",
    "        if corr < .3:\n",
    "            print(i)\n",
    "    \n",
    "    x = list(zip(total_neural_volume, healthy_dist))\n",
    "    x = sorted(x, key = lambda y: y[0])\n",
    "    TNV = [item[0] for item in x]\n",
    "    healthy_dist = [item[1] for item in x]\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    depressed_dist = []\n",
    "    for i in range(np.shape(depressed42_connectome)[0]):\n",
    "        corr = np.corrcoef(depressed42_connectome[i].flatten(), SC.flatten())[0,1]\n",
    "        depressed_dist.append(corr)\n",
    "        if corr < .2:\n",
    "            print(i)\n",
    "\n",
    "        \n",
    "    \n",
    "    control_dist = []\n",
    "    for i in range(np.shape(control_connectome)[0]):\n",
    "        corr = np.corrcoef(control_connectome[i].flatten(), SC.flatten())[0,1]\n",
    "        control_dist.append(corr)\n",
    "        if corr < .3:\n",
    "            print(i)\n",
    "    if plot:\n",
    "        plt.plot(TNV, healthy_dist)\n",
    "        plt.title(\"Healty Distribution TNV\")\n",
    "        plt.ylabel(\"Correlation\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.boxplot([healthy_dist, depressed_dist, control_dist], labels = [\"Healthy Dist\", \"Depressed Dist\", \"Control Dist\"])\n",
    "        plt.title(\"Comparing predicted Connectome from \"+ data_name + \"\\n with connectome distributions\")\n",
    "        plt.show()\n",
    "    return np.mean(healthy_dist), np.mean(depressed_dist), np.mean(control_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_dist(my_weights, \"test\")\n",
    "indexes = [18, 64]\n",
    "for num in indexes: \n",
    "    print(np.corrcoef(depressed42_connectome[num].flatten(), my_weights.flatten())[0,1])\n",
    "    plt.imshow(depressed42_connectome[num])\n",
    "    plt.show()\n",
    "np.corrcoef(depressed42_connectome[64].flatten(), depressed42_connectome[18].flatten())\n",
    "print(depressed42_connectome.shape)\n",
    "z = np.delete(depressed42_connectome, 18, axis=0)\n",
    "z = np.delete(z, 64, axis = 0)\n",
    "for item in z:\n",
    "    plt.imshow(item)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_decimal_places(num):\n",
    "    count = 0\n",
    "    while num < 1:\n",
    "        num *= 10\n",
    "        count += 1\n",
    "    return count\n",
    "1 / (10 **number_of_decimal_places(.0009))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dict = {\"hcp_246\": (large_246, 2.2, .0152), \"depressed_246\": (depressed_data, 3, .0152), \"midnight\": (midnight, 2.2, .0088)}\n",
    "data_dict = {\"hcp_246\": (large_246, .72, .000952), \"depressed_246\": (depressed_data[0:24, :, :], 3, .000552), \"control\": (control_data, 3, .00152)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# alphas\n",
    "# midnight = .0088  \n",
    "# hcp_66 = .001   \n",
    "# hcp_246 = .0152  \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "\n",
    "new_alpha_list_healthy = []\n",
    "new_alpha_list_control = []\n",
    "new_alpha_list_depressed = []\n",
    "alpha_x_axis = []\n",
    "matrix_list = []\n",
    "start_time = time.time()\n",
    "total_time = start_time\n",
    "times_alpha_sweep = 5\n",
    "\n",
    "#tr = 3\n",
    "\n",
    "\n",
    "#l for changing alpha\n",
    "#j for changing between midnight and all_data\n",
    "def algorithm(data_name, short = False, print_images = False, sweep = False, desired_length = 149, save = False):\n",
    "\n",
    "    #data_name = \"hcp_246\n",
    "    iters = 50000 if not sweep else 40000\n",
    "    current_data = data_dict[data_name][0]\n",
    "    current_scan = current_data if type(current_data) != list else current_data[0]\n",
    "    num_timepts = current_scan.shape[2] if not short else math.ceil(current_scan.shape[2] / 4)\n",
    "    num_subjects = current_scan.shape[0]\n",
    "    num_vox = current_scan.shape[1]\n",
    "    tr = data_dict[data_name][1]\n",
    "    times_alpha_sweep = 1 if not sweep else 10\n",
    "    step = 1/(10*number_of_decimal_places(data_dict[data_name][2]))\n",
    "    print(step)\n",
    "\n",
    "\n",
    "    for l in range(times_alpha_sweep):\n",
    "        for j in range(1): \n",
    "\n",
    "\n",
    "            rest = tf.placeholder(tf.float32, shape = (num_vox, num_timepts - 1))\n",
    "            dr = tf.placeholder(tf.float32, shape = (num_vox, num_timepts - 1))\n",
    "            dt = tf.constant(tr, dtype = tf.dtypes.float32)\n",
    "            id = tf.constant(np.identity(num_vox), dtype = tf.dtypes.float32)\n",
    "            length_tru = tf.constant(avg_new_lengths, dtype = tf.dtypes.float32 )\n",
    "\n",
    "            #alpha = tf.constant(0.01  +  (l + 1)/1000.0)\n",
    "            #alpha = tf.constant(.0001 + (l + 1)/100)\n",
    "            if sweep:\n",
    "                alpha = tf.constant(data_dict[data_name][2] + step * (l + 1))\n",
    "            else:\n",
    "                alpha = tf.constant(data_dict[data_name][2])\n",
    "            weight_matrix0 = tf.Variable(np.random.randn(num_vox, num_vox), dtype = tf.dtypes.float32)\n",
    "            weight_matrix1 = tf.add(weight_matrix0,tf.transpose(weight_matrix0))\n",
    "            diagonal = tf.diag(tf.diag_part(weight_matrix1))\n",
    "            weight_matrix2 = tf.subtract(weight_matrix1, diagonal)\n",
    "            weight_matrix = tf.square(weight_matrix2)\n",
    "\n",
    "            #(W-I)R\n",
    "            weight_minus_id = tf.subtract(weight_matrix, id)\n",
    "            predicted = tf.linalg.matmul(weight_minus_id, rest)\n",
    "            predicted = tf.math.scalar_mul(dt, predicted)\n",
    "\n",
    "            loss = tf.losses.mean_squared_error(dr, predicted) +\n",
    "                    tf.math.multiply(tf.norm(tf.multiply(weight_matrix, length_tru)), alpha)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate = .001).minimize(loss)\n",
    "\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess = tf.Session(config = config)\n",
    "            sess.run(init)\n",
    "            for i in range(iters):\n",
    "                random_sample, number = sample(current_data, short = short, desired_length = desired_length)\n",
    "\n",
    "                current_frame = np.delete(random_sample, num_timepts - 1, 1)\n",
    "                next_step = np.delete(random_sample, 0, 1)\n",
    "                change = np.subtract(current_frame, next_step)\n",
    "                tr_, alpha_, opt, my_weights, session_loss = sess.run(fetches = [dt, alpha, optimizer, weight_matrix, loss], \n",
    "                                                                      feed_dict = {rest: current_frame, dr: change})\n",
    "                if (i % 10000 == 0 or i == iters -1) and (times_alpha_sweep == 1 or print_images):\n",
    "                    print(l)\n",
    "                    print(i)\n",
    "                    print(data_name)\n",
    "                    print(\"alpha\", alpha_)\n",
    "                    #UNCOMMENT\n",
    "                    #print(\"Volumetric: \", np.corrcoef(weights.flatten(), my_weights.flatten())[0,1])\n",
    "                    #print(\"Surface: \", np.corrcoef(connections.flatten(), my_weights.flatten())[0,1])\n",
    "#                     print(np.shape(depressed_volumetric))\n",
    "#                     print(\"Depressed: \", np.corrcoef(depressed_volumetric.flatten(), my_weights.flatten())[0,1])\n",
    "#                     print(\"Average Healthy: \", np.corrcoef(avg_new_weights_246.flatten(), my_weights.flatten())[0,1])\n",
    "#                     print(\"Control: \", np.corrcoef(control_connectome[0].flatten(), my_weights.flatten())[0,1])\n",
    "                    \n",
    "                    if i == iters - 1:\n",
    "                        plot_dist(my_weights, data_name)\n",
    "\n",
    "\n",
    "\n",
    "                    #correlate also with new file\n",
    "                    #correlate each for i in range(66) for each row and compare it to structural matrix\n",
    "                    #only look at quad 1 and quad 4 for the new file\n",
    "                    plt.imshow(my_weights)\n",
    "                    if not path.exists(data_name + '.npy') or (save and i == iters - 1):\n",
    "                        with open(data_name + '.npy', 'wb') as f:\n",
    "                            print(\"Saved\")\n",
    "                            np.save(f, my_weights)\n",
    "                    plt.show()\n",
    "                if i == iters - 1 and (times_alpha_sweep > 1 or not print_images or sweep):\n",
    "\n",
    "                    alpha_x_axis.append(alpha_)\n",
    "                    print(data_name)\n",
    "                    print(\"alpha: \", alpha_)\n",
    "                    print(\"tr: \", tr_)\n",
    "                    print(\"step:\", step, \"l: \", l)\n",
    "\n",
    "#                     print(\"Depressed: \", np.corrcoef(depressed_volumetric.flatten(), my_weights.flatten())[0,1])\n",
    "#                     print(\"Average Healthy: \", np.corrcoef(avg_new_weights_246.flatten(), my_weights.flatten())[0,1])\n",
    "#                     print(\"Control: \", np.corrcoef(control_connectome[0].flatten(), my_weights.flatten())[0,1])\n",
    "                    \n",
    "                    #plot_dist(my_weights)\n",
    "\n",
    "                    #plt.imshow(my_weights)\n",
    "                    #plt.show()\n",
    "                \n",
    "                    healthy, depressed, control = plot_dist(my_weights, data_name, plot = print_images)\n",
    "                    new_alpha_list_depressed.append(depressed)\n",
    "                    new_alpha_list_healthy.append(healthy)\n",
    "                    new_alpha_list_control.append(control)\n",
    "\n",
    "                    if times_alpha_sweep > 1:\n",
    "                        plt.plot(np.arange(len(new_alpha_list_healthy)) * step, new_alpha_list_healthy, c = \"b\")\n",
    "                        plt.plot(np.arange(len(new_alpha_list_control)) * step, new_alpha_list_control, c = \"g\")\n",
    "                        plt.plot(np.arange(len(new_alpha_list_depressed)) * step, new_alpha_list_depressed, c = \"r\")\n",
    "                        plt.show()\n",
    "\n",
    "\n",
    "                    matrix_list.append(my_weights)\n",
    "#                     with open(data_name + '.npy', 'wb') as f:\n",
    "#                         print(\"Saved\")\n",
    "#                         np.save(f, my_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.corrcoef(depressed42_connectome[0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = []\n",
    "# for item in data_dict:\n",
    "#     algorithm(item, print_images = True, sweep = False, save = True)    \n",
    "algorithm(\"hcp_246\", sweep = True)\n",
    "\n",
    "#scipy.stats.ttest_ind T TEST BOX PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.subtract(x[2], x[1])\n",
    "y = np.subtract(x[2], x[0]\n",
    "np.corrcoef(z.flatten(), y.flatten())[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = large_246[0][0][187]\n",
    "d_dn = [d[4*x] for x in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(d_dn)\n",
    "d_dn_half = d_dn[:150]\n",
    "d_ddp = depressed_data[0][187]\n",
    "plt.plot(d_dn_half, c = \"red\")\n",
    "plt.plot(d_ddp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffrence_healthy_depressed = np.subtract(avg_new_weights_246, depressed_volumetric)\n",
    "np.savetxt(\"diffrence_healthy_depressed.txt\", diffrence_healthy_depressed, delimiter = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.add(np.add(new_alpha_list_control, new_alpha_list_depressed ), new_alpha_list_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"pt0001TOpt01alphas.txt\", new_alpha_list_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_weights_size = np.shape(my_weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(loaded_246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rsq import myrsq\n",
    "def generate_errors_and_correlation(matrix, rsq = False):\n",
    "    correlation = []\n",
    "    errors = []\n",
    "    #m = weights\n",
    "    size = matrix.shape[0]\n",
    "   \n",
    "    m = matrix\n",
    "    w = my_weights\n",
    "    #m = normalize_matrix(matrix)\n",
    "    #w = normalize_matrix(my_weights)\n",
    "    for i in range(size):\n",
    "        if i < int(size/2):\n",
    "            #correlation.append(np.corrcoef(connections[i][33:].flatten(), w[i][33:])[0,1])\n",
    "            #mean_squared_error = (np.square(connections[i][33:] - w[i][33:])).mean(axis = None)\n",
    "            #errors.append(mean_squared_error)\n",
    "\n",
    "            correlation.append(np.corrcoef(m[i][:int(size/2)].flatten(), w[i][:int(size/2)])[0,1])\n",
    "            \n",
    "            if rsq:\n",
    "                m = normalize_matrix(matrix)\n",
    "                w = normalize_matrix(my_weights)\n",
    "\n",
    "                errors.append(myrsq(m[i][:int(size/2)], w[i][:int(size/2)]))\n",
    "            else:\n",
    "                mean_squared_error = (np.square(m[i][:int(size/2)] - w[i][:int(size/2)])).mean(axis = None)\n",
    "                errors.append(mean_squared_error)\n",
    "        else:\n",
    "            #correlation.append(np.corrcoef(connections[i][:33].flatten(), w[i][:33])[0,1])\n",
    "            #mean_squared_error = (np.square(connections[i][:33] - w[i][:33])).mean(axis = None)\n",
    "            #errors.append(mean_squared_error)\n",
    "\n",
    "            correlation.append(np.corrcoef(m[i][int(size/2):].flatten(), w[i][int(size/2):])[0,1])\n",
    "            \n",
    "            if rsq:\n",
    "                m = normalize_matrix(matrix)\n",
    "                w = normalize_matrix(my_weights)\n",
    "\n",
    "                errors.append(myrsq(m[i][int(size/2):], w[i][int(size/2):]))\n",
    "            else:\n",
    "                mean_squared_error = (np.square(m[i][int(size/2):] - w[i][int(size/2):])).mean(axis = None)\n",
    "                errors.append(mean_squared_error)\n",
    "    return errors, correlation\n",
    "error, correlations = generate_errors_and_correlation(avg_new_weights_246, False)\n",
    "correlations = np.asarray(correlations)\n",
    "error = np.asarray(error)\n",
    "print(error.shape, correlations.shape)\n",
    "plt.scatter(error[:210], averages246)\n",
    "plt.title(\"Error per Region vs Average Myelin using rsq with Surface\")\n",
    "plt.xlabel(\"Error per Region\")\n",
    "#plt.ylabel(\"Average Myelin\")\n",
    "plt.ylabel(\"Mylein\")\n",
    "\n",
    "#print(np.corrcoef(error, averages))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('246Error.txt', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_percentage(matrix, percentage):\n",
    "    return_matrix = matrix\n",
    "    print(type(return_matrix), np.shape(return_matrix))\n",
    "    entries = return_matrix.flatten()\n",
    "    sorted_entries = sorted(entries, reverse = True)\n",
    "    print(\"done sorted\")\n",
    "    number = int(len(sorted_entries) * (percentage/100)) \n",
    "    sorted_entries = sorted_entries[: number]\n",
    "    convert_one_zero = lambda y: list(map(lambda x: 1 if x in sorted_entries else 0, y))\n",
    "    return_matrix = list(map(convert_one_zero, list(return_matrix)))\n",
    "#     for i in range(return_matrix.shape[0]):\n",
    "#         for j in range(return_matrix.shape[1]):\n",
    "#             current_element = return_matrix.item((i,j))\n",
    "#             return_matrix[i][j] = 1 if current_element in sorted_entries and current_element != 0 else 0\n",
    "    return return_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(my_weights)\n",
    "plt.show()\n",
    "new_array = top_percentage(np.copy(my_weights), 7.5)\n",
    "plt.imshow(new_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_thresh = top_percentage(np.copy(avg_new_weights_246), 7.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run estimated and real threashholded\n",
    "#corrcoef v_estimated[i] with v_real[j]\n",
    "a = 7.5\n",
    "actual_thresh = np.array(top_percentage(np.copy(avg_new_weights_246[:246,:246]), a))\n",
    "new_array = np.array(top_percentage(np.copy(my_weights[:246,:246]), a))\n",
    "\n",
    "\n",
    "w,v = LA.eig(0.5*(new_array+np.transpose(new_array)))\n",
    "w1, v1 = LA.eig(0.5*(actual_thresh + np.transpose(actual_thresh)))\n",
    "corr = np.zeros((246,246))\n",
    "common = np.zeros((246,246))\n",
    "for i in range(246):\n",
    "    for j in range(246):\n",
    "        corr[i,j] = np.corrcoef(v1[i], v[j])[0,1]\n",
    "        common[i,j] = 1*new_array[i,j] if new_array[i,j] == actual_thresh[i,j] else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_array = []\n",
    "for i in range(my_weights_size):\n",
    "    degree_array.append(np.sum(new_array[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees, counts = np.unique(degree_array, return_counts = True)\n",
    "counts = np.divide(counts, my_weights_size)\n",
    "together = zip(degrees, counts)\n",
    "dictionary = dict()\n",
    "for d,c in together:\n",
    "    dictionary[d] = c\n",
    "dictionary\n",
    "degrees = [dictionary[deg] for deg in degree_array ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_degree_distribution(matricies, graph_name, matrix_labels, percentage = 15):\n",
    "    return_tuple = []\n",
    "    fig, ax = plt.subplots()\n",
    "    i = 1\n",
    "    \n",
    "    count = 0\n",
    "    for matrix, label in zip(matricies, matrix_labels):\n",
    "        dictionary = dict()\n",
    "    \n",
    "            \n",
    "        new_array = np.asarray(top_percentage(matrix, percentage))\n",
    "        graph = nx.from_numpy_matrix(new_array)\n",
    "\n",
    "\n",
    "        degree_sequence = sorted([d for n, d in graph.degree()], reverse=True)  # degree sequence\n",
    "        # print \"Degree sequence\", degree_sequence\n",
    "        degreeCount = collections.Counter(degree_sequence)\n",
    "        deg, cnt = zip(*degreeCount.items())\n",
    "        np.save(\"degree.txt\", deg)\n",
    "        np.save(\"cnt.txt\", cnt)\n",
    "        cnt = np.divide(cnt, np.shape(matrix)[0])\n",
    "        \n",
    "        for d, c in zip(deg, cnt):\n",
    "            dictionary[d] = c\n",
    "        \n",
    "        \n",
    "        return_tuple.append((degree_sequence, dictionary))\n",
    "        average_deg = np.sum([degree * count for degree, count in zip(deg,cnt)])\n",
    "        #cnt = np.multiply(cnt, average_deg) \n",
    "\n",
    "    \n",
    "    #plt.bar(deg, np.divide(cnt, 66), width=0.80, color='b')\n",
    "        #plt.figure(figsize=(20,10))\n",
    "        plt.bar(deg, cnt, label = label)\n",
    "        count += 1\n",
    "        #plt.plot(deg, [np.mean(cnt)] * len(deg), label = \"matrix\" + str(i), linestyle = \"--\")\n",
    "       \n",
    "\n",
    "        #plt.plot(deg,cnt,)\n",
    "        i += 1\n",
    "\n",
    "        plt.title(\"Distribution of Top \" + str(percentage) + \" Percent of Nodes for \" + graph_name + \" Data\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Degree\")\n",
    "\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    fig.savefig('test2png.png', dpi=100)\n",
    "        \n",
    "\n",
    "#     ax.set_xticks([d + 0.4 for d in deg])\n",
    "    labels  = np.arange(24) * 5\n",
    "    ax.set_xticks([d  for d in labels])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend(bbox_to_anchor = (1.04, 1), loc = \"upper left\")\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    #fig.set_figheight(10)\n",
    "    #fig.set_figwidth(40)\n",
    "    plt.show(fig)\n",
    "    return return_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hcp_246.npy', 'rb') as f:\n",
    "    my_weights = np.load(f)\n",
    "    \n",
    "\n",
    "deg_dis = generate_degree_distribution([np.copy(normalize_matrix(my_weights)), np.copy(normalize_matrix(new_weights[0]))], \"Both\", [\"Generated SC from Healthy\", \"Healthy HCP Connectome\"], 15)\n",
    "plt.imshow(my_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('depressed_246.npy', 'rb') as f:\n",
    "    depressed = np.load(f)\n",
    "with open('control.npy', 'rb') as f:\n",
    "    control = np.load(f)\n",
    "matricies  = [[my_weights, \"hcp_246\"], [depressed, \"depressed_246\"], [control, \"control\"]]\n",
    "for item in matricies:\n",
    "    plot_dist(item[0], item[1])\n",
    "              \n",
    "print(np.corrcoef(depressed.flatten(), control.flatten())[0,1])\n",
    "print(np.corrcoef(depressed.flatten(), my_weights.flatten())[0,1])\n",
    "print(np.corrcoef(my_weights.flatten(), control.flatten())[0,1])\n",
    "#characteroze if we can use SC to see what diffrences we can make from healthy and contro\n",
    "#see the diffrences between predicted healthy and predicted contrl. want to see if these diffrences\n",
    "#are the ame or if something else if up\n",
    "#what to know how a bvery specifc region looks when we we look at healthy vs depressed. want to see if \n",
    "#we can find that out using our method. would be nice if we can find it out with our method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist(my_weights, \"tst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroed_out = np.subtract(new_weights[0], np.diag(np.diag(new_weights[0])))\n",
    "plt.imshow(normalize_matrix((zeroed_out)))\n",
    "plt.clim([-0,.1])\n",
    "\n",
    "plt.show()\n",
    "plt.imshow(normalize_matrix(my_weights))\n",
    "plt.clim([0,.1])\n",
    "\n",
    "zeroed_out[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(np.arange(246*246),my_weights.flatten()) data:image/png;base64,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\n",
    "#normalize the matirix\n",
    "\n",
    "mean = np.mean(new_weights, (0))\n",
    "mean = np.subtract(mean, np.diag(np.diag(mean)))\n",
    "\n",
    "plt.scatter(mean.flatten(), my_weights.flatten()) \n",
    "rsq = myrsq(np.mean(new_weights, (0)).flatten(), my_weights.flatten())\n",
    "plt.plot([0, 1], [0, rsq], 'k-', color = 'r')\n",
    "\n",
    "print(np.corrcoef(mean.flatten(), my_weights.flatten())[0,1])\n",
    "plt.show()\n",
    "plt.scatter(np.mean(normalize_matrix(new_weights), (0)).flatten(), normalize_matrix(my_weights).flatten()) \n",
    "\n",
    "#np.mean(new_weights, (0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsq import myrsq\n",
    "print(myrsq(np.mean(new_weights, (0)).flatten(), my_weights.flatten()))\n",
    "myrsq(np.mean(normalize_matrix(new_weights), (0)).flatten(), normalize_matrix(my_weights).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################CENTRALITY MEASURES###########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_weights_graph = nx.from_numpy_matrix(my_weights)\n",
    "degree_centrality = nx.degree_centrality(my_weights_graph)\n",
    "degree_centrality_values = list(degree_centrality.values())\n",
    "\n",
    "plt.scatter(np.arange(len(degree_centrality_values)) + 1, degree_centrality_values)\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Degree Centrality\")\n",
    "plt.title(\"Degree Centrality Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvector_centrality = nx.eigenvector_centrality(my_weights_graph)\n",
    "eigenvector_centrality_values = list(eigenvector_centrality.values())\n",
    "\n",
    "plt.scatter(np.arange(len(eigenvector_centrality_values)) + 1, eigenvector_centrality_values)\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"EigenVector Centrality\")\n",
    "plt.title(\"EigenVector Centrality Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness_centrality = nx.closeness_centrality(my_weights_graph)\n",
    "closeness_centrality_values = list(closeness_centrality.values())\n",
    "\n",
    "plt.scatter(np.arange(len(closeness_centrality_values)) + 1, closeness_centrality_values)\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Closeness Centrality\")\n",
    "plt.title(\"Closness Centrality Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_flow_closeness_centrality = nx.current_flow_closeness_centrality(my_weights_graph)\n",
    "current_flow_closeness_centrality_values = list(current_flow_closeness_centrality.values())\n",
    "\n",
    "plt.scatter(np.arange(len(current_flow_closeness_centrality_values)) + 1, current_flow_closeness_centrality_values)\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Current Flow Closeness Centrality\")\n",
    "plt.title(\"Current Flow Closness Centrality Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_centrality = nx.betweenness_centrality(my_weights_graph)\n",
    "betweenness_centrality_values = list(betweenness_centrality.values())\n",
    "\n",
    "k = list(betweenness_centrality.keys())\n",
    "v = list(betweenness_centrality.values())\n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(betweenness_centrality_values)) + 1, betweenness_centrality_values)\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Betweenness Centrality\")\n",
    "plt.title(\"Betweenness Centrality Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edge_betweenness_centrality = nx.edge_betweenness_centrality(my_weights_graph)\n",
    "edge_betweenness_centrality_values = list(edge_betweenness_centrality.values())\n",
    "\n",
    "#plt.scatter(np.arange(len(edge_betweenness_centrality_values)) + 1, edge_betweenness_centrality_values)\n",
    "cutoff = .0002\n",
    "node_dict = {}\n",
    "\n",
    "for x,y in edge_betweenness_centrality:\n",
    "    if edge_betweenness_centrality[(x,y)] > cutoff:\n",
    "        if x in node_dict:\n",
    "            node_dict[x] += 1\n",
    "        else: \n",
    "            node_dict[x] = 1\n",
    "\n",
    "k = list(node_dict.keys())\n",
    "v = list(node_dict.values())\n",
    "\n",
    "sorted_pairs = sorted([(x,y) for x,y in zip(k,v)], key = lambda z: z[1], reverse = True)\n",
    "\n",
    "plt.scatter(np.arange(len(v)), v)\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Edge Betweenness Centrality\")\n",
    "plt.title(\"Edge Betweenness Centrality Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_flow_betweenness_centrality = nx.current_flow_betweenness_centrality(my_weights_graph)\n",
    "current_flow_betweenness_centrality_values = list(current_flow_betweenness_centrality.values())\n",
    "\n",
    "k = list(current_flow_betweenness_centrality.keys())\n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(current_flow_betweenness_centrality_values)) + 1, current_flow_betweenness_centrality_values)\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Current Flow Betweenness Centrality\")\n",
    "plt.title(\"Current Flow Betweenness Centrality Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_dis = generate_degree_distribution([np.copy(my_weights), np.copy(depressed_volumetric)], \"Both\", 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MIGHT NOT WORK WITH 246\n",
    "#deg_dis_surface = generate_degree_distribution([np.copy(my_weights[:my_weights_size,:my_weights_size]), np.copy(weights[:32,:32]), np.copy(connections[:32, :32])], \"Both\", 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dict()\n",
    "degree, count = deg_dis[0][0], deg_dis[0][1]\n",
    "for d, c in zip(degree, count):\n",
    "    dictionary[d] = c\n",
    "dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_node_scatter(x,y, xlabel:str, ylabel:str, title:str, cutoff = 0):\n",
    "    for i in range(len(error)):\n",
    "        if error[i] < cutoff and i < my_weights_size/2: \n",
    "            plt.scatter(x[i], y[i], label = values[i])\n",
    "            plt.annotate(keys[i], (x[i], y[i]))\n",
    "        elif i >my_weights_size/2 and x[i] < cutoff:\n",
    "            plt.scatter(x[i], y[i], label = values[my_weights_size - i])\n",
    "            plt.annotate(keys[my_weights_size - i], (x[i], y[i]))\n",
    "        else:\n",
    "            plt.scatter(x[i], y[i], c = \"blue\")\n",
    "    #plt.legend(bbox_to_anchor = (1.04, 1), loc = \"upper left\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#error, correlations = generate_errors_and_correlation(averages246, rsq = False)\n",
    "title = \"Results using RSQ with the Volumetric Matrix\"\n",
    "show_node_scatter(error, degrees, \"Errors\", \"Mylin\", title)\n",
    "show_node_scatter(error, degree_array, \"Errors\", \"Degree\", title)\n",
    "show_node_scatter(correlations, degree_array, \"Correlation\", \"Degree\",title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error, correlations = generate_errors_and_correlation(connections, True)\n",
    "\n",
    "title = \"Results using RSQ with the Surface Matrix\"\n",
    "show_node_scatter(error, averages, \"Errors\", \"Mylin\", title)\n",
    "show_node_scatter(error, degree_array, \"Errors\", \"Degree\", title)\n",
    "show_node_scatter(correlations, degree_array, \"Correlation\", \"Degree\",title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error, correlations = generate_errors_and_correlation(weights, False)\n",
    "\n",
    "title = \"Results using MSE with the Volumetric Matrix\"\n",
    "show_node_scatter(error, averages, \"Errors\", \"Mylin\", title)\n",
    "show_node_scatter(error, degree_array, \"Errors\", \"Degree\", title)\n",
    "show_node_scatter(correlations, degree_array, \"Correlation\", \"Degree\",title)\n",
    "aver_er == error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error, correlations = generate_errors_and_correlation(connections, False)\n",
    "\n",
    "title = \"Results using MSE with the Surface Matrix\"\n",
    "show_node_scatter(error, averages, \"Errors\", \"Mylin\", title)\n",
    "show_node_scatter(error, degree_array, \"Errors\", \"Degree\", title)\n",
    "show_node_scatter(correlations, degree_array, \"Correlation\", \"Degree\",title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.empty([2,2])\n",
    "new_col = np.zeros([x.shape[1], 1])\n",
    "new_row = np.zeros([1, x.shape[0] + 1])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.hstack((x, new_col))\n",
    "# print(x)\n",
    "# print(x.shape)\n",
    "# print(new_row.shape)\n",
    "# y = np.vstack((x, new_row))\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_matrix(mtrx):\n",
    "    cpy = np.copy(mtrx)\n",
    "    new_col = np.zeros([mtrx.shape[1], 1])\n",
    "    new_row = np.zeros([1, mtrx.shape[0] + 1])\n",
    "    cpy = np.hstack((cpy, new_col))\n",
    "    cpy = np.vstack((cpy, new_row))\n",
    "    return cpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    y = increase_matrix(y)\n",
    "y[3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "def plotHist(x, isMidnight =  False):\n",
    "    \n",
    "    matrix = normalize_matrix(x)\n",
    "    v = normalize_matrix(weights)\n",
    "    s = normalize_matrix(connections)\n",
    "    flattened = []\n",
    "    flattened2 = []\n",
    "    vol = []\n",
    "    vol2 = []\n",
    "    surf = []\n",
    "    surf2 = []\n",
    "\n",
    "    for i in range(33):\n",
    "        flattened.append(matrix[i][:33].flatten())\n",
    "        flattened.append(matrix[65 - i][33:].flatten())\n",
    "        vol.append(v[i][:33].flatten())\n",
    "        vol.append(v[65 - i][33:].flatten()) \n",
    "        surf.append(s[i][:33].flatten())\n",
    "        surf.append(s[65 - i][33:].flatten())\n",
    "        \n",
    "        flattened2.append(matrix[i][33:].flatten())\n",
    "        flattened2.append(matrix[65 - i][:33].flatten())\n",
    "        vol2.append(v[i][33:].flatten())\n",
    "        vol2.append(v[65 - i][:33].flatten())\n",
    "        surf2.append(s[i][33:].flatten())\n",
    "        surf2.append(s[65 - i][:33].flatten())\n",
    "        \n",
    "    flattened =  np.asarray(flattened).flatten()\n",
    "    flattened = list(filter(lambda a: a >.1 , flattened))\n",
    "    vol =  np.asarray(vol).flatten()\n",
    "    vol = list(filter(lambda a: a >.1 , vol))\n",
    "    surf =  np.asarray(surf).flatten()\n",
    "    surf = list(filter(lambda a: a >.1 , surf))\n",
    "    flattened2 =  np.asarray(flattened2).flatten()\n",
    "    flattened2 = list(filter(lambda a: a >.1 , flattened2))\n",
    "    vol2 =  np.asarray(vol2).flatten()\n",
    "    vol2 = list(filter(lambda a: a >.1 , vol2))\n",
    "    surf2 =  np.asarray(surf2).flatten()\n",
    "    surf2 = list(filter(lambda a: a >.1 , surf2))\n",
    "    \n",
    "    fig = pyplot.figure(figsize = (20,10))\n",
    "    ax1 = fig.add_subplot(2,2,1)\n",
    "    ax2 = fig.add_subplot(2,2,2)\n",
    "    ax3 = fig.add_subplot(2,2,3)\n",
    "    ax4 = fig.add_subplot(2,2,4)\n",
    "    \n",
    "    subplots = [ax1, ax2, ax3, ax4]\n",
    "    \n",
    "    ax1.hist(flattened, weights=np.ones(len(flattened)) / len(flattened), alpha = .5, bins = 20, label = \"estimated\")\n",
    "    ax2.hist(flattened, weights=np.ones(len(flattened)) / len(flattened), alpha = .5, bins = 20, label = \"estimated\")\n",
    "    ax3.hist(flattened2, weights=np.ones(len(flattened2)) / len(flattened2), alpha = .5, bins = 20, label = \"estimated\")\n",
    "    ax4.hist(flattened2, weights=np.ones(len(flattened2)) / len(flattened2), alpha = .5, bins = 20, label = \"estimated\")\n",
    "\n",
    "    ax1.hist(vol, weights=np.ones(len(vol)) / len(vol), alpha = .5, bins = 20, label = \"volumetric\")\n",
    "    ax2.hist(surf, weights=np.ones(len(surf)) / len(surf), alpha = .5, bins = 20, label = \"surface\")\n",
    "    \n",
    "    ax3.hist(vol2, weights=np.ones(len(vol2)) / len(vol2), alpha = .5, bins = 20, label = \"volumetric\")\n",
    "    ax4.hist(surf2, weights=np.ones(len(surf2)) / len(surf2), alpha = .5, bins = 20, label = \"surface\")\n",
    "\n",
    "    \n",
    "\n",
    "    #pyplot.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "    if isMidnight:\n",
    "        ax1.set_title(\"Estimated from Midnight 1st and 4th Quartiles\")\n",
    "        ax2.set_title(\"Estimated from Midnight 1st and 4th Quartiles\")        \n",
    "        ax3.set_title(\"Estimated from Midnight 2nd and 3rd Quartiles\")\n",
    "        ax4.set_title(\"Estimated from Midnight 2nd and 3rd Quartiles\")\n",
    "\n",
    "    else:\n",
    "        ax1.set_title(\"Estimated from HRC 1st and 4th Quartiles\")\n",
    "        ax2.set_title(\"Estimated from HRC 1st and 4th Quartiles\")   \n",
    "        ax3.set_title(\"Estimated from HRC 2nd and 3rd Quartiles\")\n",
    "        ax4.set_title(\"Estimated from HRC 2nd and 3rd Quartiles\")\n",
    "\n",
    "\n",
    "    \n",
    "           \n",
    "\n",
    "    ax1.set_xlabel(\"Weight of Connection\")\n",
    "    ax2.set_xlabel(\"Weight of Connection\")\n",
    "    ax3.set_xlabel(\"Weight of Connection\")\n",
    "    ax4.set_xlabel(\"Weight of Connection\")\n",
    "    ax1.set_ylabel(\"Percentage\")\n",
    "    ax2.set_ylabel(\"Percentage\")\n",
    "    ax3.set_ylabel(\"Percentage\")\n",
    "    ax4.set_ylabel(\"Percentage\")\n",
    "\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax3.legend(loc='upper right')\n",
    "    ax4.legend(loc='upper right')\n",
    "    \n",
    "    for plot in subplots:\n",
    "        vals = plot.get_yticks()\n",
    "        plot.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n",
    "    fig.show()\n",
    "    return flattened\n",
    "#x = plotHist(my_weights, False, True, False)\n",
    "#y = plotHist(my_weights, True, True, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
